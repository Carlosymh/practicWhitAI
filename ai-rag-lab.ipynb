{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c44b9c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pymongo import MongoClient\n",
    "from utils import track_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4780396c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ok': 1}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you are using your own MongoDB Atlas cluster, use the connection string for your cluster here\n",
    "MONGODB_URI = os.environ.get(\"MONGODB_URI\")\n",
    "# Initialize a MongoDB Python client\n",
    "mongodb_client = MongoClient(MONGODB_URI)\n",
    "# Check the connection to the server\n",
    "mongodb_client.admin.command(\"ping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5876f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking progress for task cluster_creation\n"
     ]
    }
   ],
   "source": [
    "# Track progress of key steps-- DO NOT CHANGE\n",
    "track_progress(\"cluster_creation\", \"ai_rag_lab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "024428ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVERLESS_URL = os.environ.get(\"SERVERLESS_URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc76bb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50341a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mongodb_docs.json\", \"r\") as data_file:\n",
    "    json_data = data_file.read()\n",
    "\n",
    "docs = json.loads(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70e3c381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the number of documents in the dataset\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1115e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'updated': '2024-05-20T17:30:49.148Z',\n",
       " 'metadata': {'contentType': None,\n",
       "  'productName': 'MongoDB Atlas',\n",
       "  'tags': ['atlas', 'docs'],\n",
       "  'version': None},\n",
       " 'action': 'created',\n",
       " 'sourceName': 'snooty-cloud-docs',\n",
       " 'body': '# View Database Access History\\n\\n- This feature is not available for `M0` free clusters, `M2`, and `M5` clusters. To learn more, see Atlas M0 (Free Cluster), M2, and M5 Limits.\\n\\n- This feature is not supported on Serverless instances at this time. To learn more, see Serverless Instance Limitations.\\n\\n## Overview\\n\\nAtlas parses the MongoDB database logs to collect a list of authentication requests made against your clusters through the following methods:\\n\\n- `mongosh`\\n\\n- Compass\\n\\n- Drivers\\n\\nAuthentication requests made with API Keys through the Atlas Administration API are not logged.\\n\\nAtlas logs the following information for each authentication request within the last 7 days:\\n\\n<table>\\n<tr>\\n<th id=\"Field\">\\nField\\n\\n</th>\\n<th id=\"Description\">\\nDescription\\n\\n</th>\\n</tr>\\n<tr>\\n<td headers=\"Field\">\\nTimestamp\\n\\n</td>\\n<td headers=\"Description\">\\nThe date and time of the authentication request.\\n\\n</td>\\n</tr>\\n<tr>\\n<td headers=\"Field\">\\nUsername\\n\\n</td>\\n<td headers=\"Description\">\\nThe username associated with the database user who made the authentication request.\\n\\nFor LDAP usernames, the UI displays the resolved LDAP name. Hover over the name to see the full LDAP username.\\n\\n</td>\\n</tr>\\n<tr>\\n<td headers=\"Field\">\\nIP Address\\n\\n</td>\\n<td headers=\"Description\">\\nThe IP address of the machine that sent the authentication request.\\n\\n</td>\\n</tr>\\n<tr>\\n<td headers=\"Field\">\\nHost\\n\\n</td>\\n<td headers=\"Description\">\\nThe target server that processed the authentication request.\\n\\n</td>\\n</tr>\\n<tr>\\n<td headers=\"Field\">\\nAuthentication Source\\n\\n</td>\\n<td headers=\"Description\">\\nThe database that the authentication request was made against. `admin` is the authentication source for SCRAM-SHA users and `$external` for LDAP users.\\n\\n</td>\\n</tr>\\n<tr>\\n<td headers=\"Field\">\\nAuthentication Result\\n\\n</td>\\n<td headers=\"Description\">\\nThe success or failure of the authentication request. A reason code is displayed for the failed authentication requests.\\n\\n</td>\\n</tr>\\n</table>Authentication requests are pre-sorted by descending timestamp with 25 entries per page.\\n\\n### Logging Limitations\\n\\nIf a cluster experiences an activity spike and generates an extremely large quantity of log messages, Atlas may stop collecting and storing new logs for a period of time.\\n\\nLog analysis rate limits apply only to the Performance Advisor UI, the Query Insights UI, the Access Tracking UI, and the Atlas Search Query Analytics UI. Downloadable log files are always complete.\\n\\nIf authentication requests occur during a period when logs are not collected, they will not appear in the database access history.\\n\\n## Required Access\\n\\nTo view database access history, you must have `Project Owner` or `Organization Owner` access to Atlas.\\n\\n## Procedure\\n\\n<Tabs>\\n\\n<Tab name=\"Atlas CLI\">\\n\\nTo return the access logs for a cluster using the Atlas CLI, run the following command:\\n\\n```sh\\n\\natlas accessLogs list [options]\\n\\n```\\n\\nTo learn more about the command syntax and parameters, see the Atlas CLI documentation for atlas accessLogs list.\\n\\n- Install the Atlas CLI\\n\\n- Connect to the Atlas CLI\\n\\n</Tab>\\n\\n<Tab name=\"Atlas Administration API\">\\n\\nTo view the database access history using the API, see Access Tracking.\\n\\n</Tab>\\n\\n<Tab name=\"Atlas UI\">\\n\\nUse the following procedure to view your database access history using the Atlas UI:\\n\\n### Navigate to the Clusters page for your project.\\n\\n- If it is not already displayed, select the organization that contains your desired project from the  Organizations menu in the navigation bar.\\n\\n- If it is not already displayed, select your desired project from the Projects menu in the navigation bar.\\n\\n- If the Clusters page is not already displayed, click Database in the sidebar.\\n\\n### View the cluster\\'s database access history.\\n\\n- On the cluster card, click .\\n\\n- Select View Database Access History.\\n\\nor\\n\\n- Click the cluster name.\\n\\n- Click .\\n\\n- Select View Database Access History.\\n\\n</Tab>\\n\\n</Tabs>\\n\\n',\n",
       " 'url': 'https://mongodb.com/docs/atlas/access-tracking/',\n",
       " 'format': 'md',\n",
       " 'title': 'View Database Access History'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview a document to understand its structure\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69c0602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec0c5ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common list of separators for text data\n",
    "separators = [\"\\n\\n\", \"\\n\", \" \", \"\", \"#\", \"##\", \"###\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1dac4803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://aws:****@trips-611463197093.d.codeartifact.us-west-2.amazonaws.com/pypi/trips-python-libs/simple/, https://pypi.org/simple\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.11.0-cp313-cp313-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Using cached regex-2025.7.34-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\carlos yovani\\projects\\practicwhitai\\venv\\lib\\site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\carlos yovani\\projects\\practicwhitai\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\carlos yovani\\projects\\practicwhitai\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\carlos yovani\\projects\\practicwhitai\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\carlos yovani\\projects\\practicwhitai\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.8.3)\n",
      "Downloading tiktoken-0.11.0-cp313-cp313-win_amd64.whl (883 kB)\n",
      "   ---------------------------------------- 0.0/883.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/883.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 883.9/883.9 kB 3.6 MB/s eta 0:00:00\n",
      "Using cached regex-2025.7.34-cp313-cp313-win_amd64.whl (275 kB)\n",
      "Installing collected packages: regex, tiktoken\n",
      "\n",
      "   -------------------- ------------------- 1/2 [tiktoken]\n",
      "   ---------------------------------------- 2/2 [tiktoken]\n",
      "\n",
      "Successfully installed regex-2025.7.34 tiktoken-0.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: 401 Error, Credentials not correct for https://trips-611463197093.d.codeartifact.us-west-2.amazonaws.com/pypi/trips-python-libs/simple/tiktoken/\n",
      "WARNING: 401 Error, Credentials not correct for https://trips-611463197093.d.codeartifact.us-west-2.amazonaws.com/pypi/trips-python-libs/simple/regex/\n",
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken\n",
    "\n",
    "# Use the `RecursiveCharacterTextSplitter` from LangChain to first split a piece of text on the list of `separators` above.\n",
    "# Then recursively merge them into tokens until the specified chunk size is reached.\n",
    "# For text data, you typically want to keep 1-2 paragraphs (~200 tokens) in a single chunk.\n",
    "# Chunk overlap of 15-20% of the chunk size is recommended to maintain context between chunks.\n",
    "# The `model_name` parameter indicates which encoder to use for tokenization, in this case GPT-4's encoder.\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=\"gpt-4\", separators=separators, chunk_size=200, chunk_overlap=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83542f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(doc: Dict, text_field: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Chunk up a document.\n",
    "\n",
    "    Args:\n",
    "        doc (Dict): Parent document to generate chunks from.\n",
    "        text_field (str): Text field to chunk.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: List of chunked documents.\n",
    "    \"\"\"\n",
    "    # Extract the field to chunk from `doc`\n",
    "    text = doc[text_field]\n",
    "    # Split `text` using the `split_text` method of the `text_splitter` object\n",
    "    # NOTE: `text` is a string\n",
    "    chunks = text_splitter.split_text(text)\n",
    "\n",
    "    # Iterate through `chunks` and for each chunk:\n",
    "    # 1. Create a shallow copy of `doc`, call it `temp`\n",
    "    # 2. Set the `text_field` field in `temp` to the content of the chunk\n",
    "    # 3. Append `temp` to `chunked_data`\n",
    "    chunked_data = []\n",
    "    for chunk in chunks:\n",
    "        temp = doc.copy()\n",
    "        temp[text_field] = chunk\n",
    "        chunked_data.append(temp)\n",
    "\n",
    "    return chunked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce862ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs = []\n",
    "# Iterate through `docs`, use the `get_chunks` function to chunk up the \"body\" field in the documents, and add the list of chunked documents to `split_docs` initialized above.\n",
    "for doc in docs:\n",
    "    chunks = get_chunks(doc, \"body\")\n",
    "    split_docs.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8748c7bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice that the length of `split_docs` is greater than the length of `docs` from Step 2 above\n",
    "# This is because each document in `docs` has been split into multiple chunks\n",
    "len(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99e06642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'updated': '2024-05-20T17:30:49.148Z',\n",
       " 'metadata': {'contentType': None,\n",
       "  'productName': 'MongoDB Atlas',\n",
       "  'tags': ['atlas', 'docs'],\n",
       "  'version': None},\n",
       " 'action': 'created',\n",
       " 'sourceName': 'snooty-cloud-docs',\n",
       " 'body': '# View Database Access History\\n\\n- This feature is not available for `M0` free clusters, `M2`, and `M5` clusters. To learn more, see Atlas M0 (Free Cluster), M2, and M5 Limits.\\n\\n- This feature is not supported on Serverless instances at this time. To learn more, see Serverless Instance Limitations.\\n\\n## Overview\\n\\nAtlas parses the MongoDB database logs to collect a list of authentication requests made against your clusters through the following methods:\\n\\n- `mongosh`\\n\\n- Compass\\n\\n- Drivers\\n\\nAuthentication requests made with API Keys through the Atlas Administration API are not logged.\\n\\nAtlas logs the following information for each authentication request within the last 7 days:\\n\\n<table>\\n<tr>\\n<th id=\"Field\">\\nField\\n\\n</th>\\n<th id=\"Description\">\\nDescription\\n\\n</th>\\n</tr>\\n<tr>\\n<td headers=\"Field\">\\nTimestamp',\n",
       " 'url': 'https://mongodb.com/docs/atlas/access-tracking/',\n",
       " 'format': 'md',\n",
       " 'title': 'View Database Access History'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview a chunked document to understand its structure\n",
    "# Note that the structure looks similar to the original docs, except the `body` field now contains smaller chunks of text\n",
    "split_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b7f282f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://aws:****@trips-611463197093.d.codeartifact.us-west-2.amazonaws.com/pypi/trips-python-libs/simple/, https://pypi.org/simple\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Using cached transformers-4.55.3-py3-none-any.whl.metadata (41 kB)\n",
      "Collecting tqdm (from sentence-transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Using cached torch-2.8.0-cp313-cp313-win_amd64.whl.metadata (30 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Using cached scikit_learn-1.7.1-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Using cached scipy-1.16.1-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Using cached huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting Pillow (from sentence-transformers)\n",
      "  Using cached pillow-11.3.0-cp313-cp313-win_amd64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\carlos yovani\\projects\\practicwhitai\\venv\\lib\\site-packages (from sentence-transformers) (4.14.1)\n",
      "Collecting filelock (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting numpy>=1.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading numpy-2.3.2-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\carlos yovani\\projects\\practicwhitai\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\carlos yovani\\projects\\practicwhitai\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\carlos yovani\\projects\\practicwhitai\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in c:\\users\\carlos yovani\\projects\\practicwhitai\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached tokenizers-0.21.4-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting setuptools (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\carlos yovani\\projects\\practicwhitai\\venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached MarkupSafe-3.0.2-cp313-cp313-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\carlos yovani\\projects\\practicwhitai\\venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\carlos yovani\\projects\\practicwhitai\\venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\carlos yovani\\projects\\practicwhitai\\venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\carlos yovani\\projects\\practicwhitai\\venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.8.3)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "Using cached transformers-4.55.3-py3-none-any.whl (11.3 MB)\n",
      "Using cached huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "Using cached tokenizers-0.21.4-cp39-abi3-win_amd64.whl (2.5 MB)\n",
      "Using cached fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "Downloading numpy-2.3.2-cp313-cp313-win_amd64.whl (12.8 MB)\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.8/12.8 MB 6.0 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 2.1/12.8 MB 4.2 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.4/12.8 MB 3.4 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.6/12.8 MB 2.9 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.9/12.8 MB 2.7 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 3.7/12.8 MB 2.7 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 4.5/12.8 MB 3.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 5.0/12.8 MB 2.9 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 6.0/12.8 MB 3.0 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 7.3/12.8 MB 3.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 8.4/12.8 MB 3.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 9.4/12.8 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.2/12.8 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.5/12.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 11.0/12.8 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.8/12.8 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.3/12.8 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.8/12.8 MB 3.3 MB/s eta 0:00:00\n",
      "Using cached safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "Using cached torch-2.8.0-cp313-cp313-win_amd64.whl (241.3 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp313-cp313-win_amd64.whl (15 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached pillow-11.3.0-cp313-cp313-win_amd64.whl (7.0 MB)\n",
      "Using cached scikit_learn-1.7.1-cp313-cp313-win_amd64.whl (8.7 MB)\n",
      "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached scipy-1.16.1-cp313-cp313-win_amd64.whl (38.5 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Installing collected packages: mpmath, tqdm, threadpoolctl, sympy, setuptools, safetensors, Pillow, numpy, networkx, MarkupSafe, joblib, fsspec, filelock, scipy, jinja2, huggingface-hub, torch, tokenizers, scikit-learn, transformers, sentence-transformers\n",
      "\n",
      "   ----------------------------------------  0/21 [mpmath]\n",
      "   ----------------------------------------  0/21 [mpmath]\n",
      "   ----------------------------------------  0/21 [mpmath]\n",
      "   ----------------------------------------  0/21 [mpmath]\n",
      "   - --------------------------------------  1/21 [tqdm]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ----- ----------------------------------  3/21 [sympy]\n",
      "   ------- --------------------------------  4/21 [setuptools]\n",
      "   ------- --------------------------------  4/21 [setuptools]\n",
      "   ------- --------------------------------  4/21 [setuptools]\n",
      "   ------- --------------------------------  4/21 [setuptools]\n",
      "   ------- --------------------------------  4/21 [setuptools]\n",
      "   ------- --------------------------------  4/21 [setuptools]\n",
      "   ------- --------------------------------  4/21 [setuptools]\n",
      "   ------- --------------------------------  4/21 [setuptools]\n",
      "   ------- --------------------------------  4/21 [setuptools]\n",
      "   ------- --------------------------------  4/21 [setuptools]\n",
      "   ------- --------------------------------  4/21 [setuptools]\n",
      "   ------- --------------------------------  4/21 [setuptools]\n",
      "   ------- --------------------------------  4/21 [setuptools]\n",
      "   ------- --------------------------------  4/21 [setuptools]\n",
      "   ------- --------------------------------  4/21 [setuptools]\n",
      "   ------- --------------------------------  4/21 [setuptools]\n",
      "   ------- --------------------------------  4/21 [setuptools]\n",
      "   ----------- ----------------------------  6/21 [Pillow]\n",
      "   ----------- ----------------------------  6/21 [Pillow]\n",
      "   ----------- ----------------------------  6/21 [Pillow]\n",
      "   ----------- ----------------------------  6/21 [Pillow]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   --------------- ------------------------  8/21 [networkx]\n",
      "   --------------- ------------------------  8/21 [networkx]\n",
      "   --------------- ------------------------  8/21 [networkx]\n",
      "   --------------- ------------------------  8/21 [networkx]\n",
      "   --------------- ------------------------  8/21 [networkx]\n",
      "   --------------- ------------------------  8/21 [networkx]\n",
      "   --------------- ------------------------  8/21 [networkx]\n",
      "   --------------- ------------------------  8/21 [networkx]\n",
      "   --------------- ------------------------  8/21 [networkx]\n",
      "   --------------- ------------------------  8/21 [networkx]\n",
      "   --------------- ------------------------  8/21 [networkx]\n",
      "   --------------- ------------------------  8/21 [networkx]\n",
      "   --------------- ------------------------  8/21 [networkx]\n",
      "   --------------- ------------------------  8/21 [networkx]\n",
      "   --------------- ------------------------  8/21 [networkx]\n",
      "   --------------- ------------------------  8/21 [networkx]\n",
      "   --------------- ------------------------  8/21 [networkx]\n",
      "   --------------- ------------------------  8/21 [networkx]\n",
      "   --------------- ------------------------  8/21 [networkx]\n",
      "   --------------- ------------------------  8/21 [networkx]\n",
      "   --------------- ------------------------  8/21 [networkx]\n",
      "   --------------- ------------------------  8/21 [networkx]\n",
      "   --------------- ------------------------  8/21 [networkx]\n",
      "   --------------- ------------------------  8/21 [networkx]\n",
      "   ------------------- -------------------- 10/21 [joblib]\n",
      "   ------------------- -------------------- 10/21 [joblib]\n",
      "   ------------------- -------------------- 10/21 [joblib]\n",
      "   -------------------- ------------------- 11/21 [fsspec]\n",
      "   -------------------- ------------------- 11/21 [fsspec]\n",
      "   -------------------- ------------------- 11/21 [fsspec]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   ------------------------ --------------- 13/21 [scipy]\n",
      "   -------------------------- ------------- 14/21 [jinja2]\n",
      "   -------------------------- ------------- 14/21 [jinja2]\n",
      "   -------------------------- ------------- 14/21 [jinja2]\n",
      "   -------------------------- ------------- 14/21 [jinja2]\n",
      "   ---------------------------- ----------- 15/21 [huggingface-hub]\n",
      "   ---------------------------- ----------- 15/21 [huggingface-hub]\n",
      "   ---------------------------- ----------- 15/21 [huggingface-hub]\n",
      "   ---------------------------- ----------- 15/21 [huggingface-hub]\n",
      "   ---------------------------- ----------- 15/21 [huggingface-hub]\n",
      "   ---------------------------- ----------- 15/21 [huggingface-hub]\n",
      "   ---------------------------- ----------- 15/21 [huggingface-hub]\n",
      "   ---------------------------- ----------- 15/21 [huggingface-hub]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   ------------------------------ --------- 16/21 [torch]\n",
      "   -------------------------------- ------- 17/21 [tokenizers]\n",
      "   -------------------------------- ------- 17/21 [tokenizers]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ---------------------------------- ----- 18/21 [scikit-learn]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   ------------------------------------ --- 19/21 [transformers]\n",
      "   -------------------------------------- - 20/21 [sentence-transformers]\n",
      "   -------------------------------------- - 20/21 [sentence-transformers]\n",
      "   -------------------------------------- - 20/21 [sentence-transformers]\n",
      "   -------------------------------------- - 20/21 [sentence-transformers]\n",
      "   -------------------------------------- - 20/21 [sentence-transformers]\n",
      "   -------------------------------------- - 20/21 [sentence-transformers]\n",
      "   -------------------------------------- - 20/21 [sentence-transformers]\n",
      "   ---------------------------------------- 21/21 [sentence-transformers]\n",
      "\n",
      "Successfully installed MarkupSafe-3.0.2 Pillow-11.3.0 filelock-3.19.1 fsspec-2025.7.0 huggingface-hub-0.34.4 jinja2-3.1.6 joblib-1.5.1 mpmath-1.3.0 networkx-3.5 numpy-2.3.2 safetensors-0.6.2 scikit-learn-1.7.1 scipy-1.16.1 sentence-transformers-5.1.0 setuptools-80.9.0 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.21.4 torch-2.8.0 tqdm-4.67.1 transformers-4.55.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: 401 Error, Credentials not correct for https://trips-611463197093.d.codeartifact.us-west-2.amazonaws.com/pypi/trips-python-libs/simple/sentence-transformers/\n",
      "WARNING: 401 Error, Credentials not correct for https://trips-611463197093.d.codeartifact.us-west-2.amazonaws.com/pypi/trips-python-libs/simple/transformers/\n",
      "WARNING: 401 Error, Credentials not correct for https://trips-611463197093.d.codeartifact.us-west-2.amazonaws.com/pypi/trips-python-libs/simple/tqdm/\n",
      "WARNING: 401 Error, Credentials not correct for https://trips-611463197093.d.codeartifact.us-west-2.amazonaws.com/pypi/trips-python-libs/simple/torch/\n",
      "WARNING: 401 Error, Credentials not correct for https://trips-611463197093.d.codeartifact.us-west-2.amazonaws.com/pypi/trips-python-libs/simple/scikit-learn/\n",
      "WARNING: 401 Error, Credentials not correct for https://trips-611463197093.d.codeartifact.us-west-2.amazonaws.com/pypi/trips-python-libs/simple/scipy/\n",
      "WARNING: 401 Error, Credentials not correct for https://trips-611463197093.d.codeartifact.us-west-2.amazonaws.com/pypi/trips-python-libs/simple/huggingface-hub/\n",
      "WARNING: 401 Error, Credentials not correct for https://trips-611463197093.d.codeartifact.us-west-2.amazonaws.com/pypi/trips-python-libs/simple/pillow/\n",
      "WARNING: 401 Error, Credentials not correct for https://trips-611463197093.d.codeartifact.us-west-2.amazonaws.com/pypi/trips-python-libs/simple/filelock/\n",
      "WARNING: 401 Error, Credentials not correct for https://trips-611463197093.d.codeartifact.us-west-2.amazonaws.com/pypi/trips-python-libs/simple/numpy/\n",
      "WARNING: 401 Error, Credentials not correct for https://trips-611463197093.d.codeartifact.us-west-2.amazonaws.com/pypi/trips-python-libs/simple/tokenizers/\n",
      "WARNING: 401 Error, Credentials not correct for https://trips-611463197093.d.codeartifact.us-west-2.amazonaws.com/pypi/trips-python-libs/simple/safetensors/\n",
      "WARNING: 401 Error, Credentials not correct for https://trips-611463197093.d.codeartifact.us-west-2.amazonaws.com/pypi/trips-python-libs/simple/fsspec/\n",
      "WARNING: 401 Error, Credentials not correct for https://trips-611463197093.d.codeartifact.us-west-2.amazonaws.com/pypi/trips-python-libs/simple/sympy/\n",
      "WARNING: 401 Error, Credentials not correct for https://trips-611463197093.d.codeartifact.us-west-2.amazonaws.com/pypi/trips-python-libs/simple/networkx/\n",
      "WARNING: 401 Error, Credentials not correct for https://trips-611463197093.d.codeartifact.us-west-2.amazonaws.com/pypi/trips-python-libs/simple/jinja2/\n",
      "WARNING: 401 Error, Credentials not correct for https://trips-611463197093.d.codeartifact.us-west-2.amazonaws.com/pypi/trips-python-libs/simple/setuptools/\n",
      "WARNING: 401 Error, Credentials not correct for https://trips-611463197093.d.codeartifact.us-west-2.amazonaws.com/pypi/trips-python-libs/simple/mpmath/\n",
      "WARNING: 401 Error, Credentials not correct for https://trips-611463197093.d.codeartifact.us-west-2.amazonaws.com/pypi/trips-python-libs/simple/markupsafe/\n",
      "WARNING: 401 Error, Credentials not correct for https://trips-611463197093.d.codeartifact.us-west-2.amazonaws.com/pypi/trips-python-libs/simple/joblib/\n",
      "WARNING: 401 Error, Credentials not correct for https://trips-611463197093.d.codeartifact.us-west-2.amazonaws.com/pypi/trips-python-libs/simple/threadpoolctl/\n",
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "c:\\Users\\Carlos Yovani\\projects\\practicWhitAI\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence-transformers\n",
    "\n",
    "# You may see a warning upon running this cell. You can ignore it.\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ecbe42b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the `gte-small` model using the Sentence Transformers library\n",
    "embedding_model = SentenceTransformer(\"thenlper/gte-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32f0cf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that takes a piece of text (`text`) as input, embeds it using the `embedding_model` instantiated above and returns the embedding as a list\n",
    "# An array can be converted to a list using the `tolist()` method\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Generate the embedding for a piece of text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Text to embed.\n",
    "\n",
    "    Returns:\n",
    "        List[float]: Embedding of the text as a list.\n",
    "    \"\"\"\n",
    "    embedding = embedding_model.encode(text)\n",
    "    return embedding.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72734fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 107/107 [00:05<00:00, 19.07it/s]\n"
     ]
    }
   ],
   "source": [
    "embedded_docs = []\n",
    "# Add an `embedding` field to each dictionary in `split_docs`\n",
    "# The `embedding` field should correspond to the embedding of the value of the `body` field\n",
    "# Use the `get_embedding` function defined above to generate the embedding\n",
    "# Append the updated dictionaries to `embedded_docs` initialized above.\n",
    "for doc in tqdm(split_docs):\n",
    "    doc[\"embedding\"] = get_embedding(doc[\"body\"])\n",
    "    embedded_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cfc43590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the length of `embedded_docs` is the same as that of `split_docs`\n",
    "len(embedded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "799c378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the database -- Change if needed or leave as is\n",
    "DB_NAME = \"mongodb_genai_devday_rag\"\n",
    "# Name of the collection -- Change if needed or leave as is\n",
    "COLLECTION_NAME = \"knowledge_base\"\n",
    "# Name of the vector search index -- Change if needed or leave as is\n",
    "ATLAS_VECTOR_SEARCH_INDEX_NAME = \"vector_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9fa10b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the `COLLECTION_NAME` collection.\n",
    "collection = mongodb_client[DB_NAME][COLLECTION_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e60e0bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteResult({'n': 107, 'electionId': ObjectId('7fffffff000000000000017a'), 'opTime': {'ts': Timestamp(1755838513, 27), 't': 378}, 'ok': 1.0, '$clusterTime': {'clusterTime': Timestamp(1755838513, 27), 'signature': {'hash': b'I\\xfb\\x7f[v\\x95\\x9b\\xc9Z\\xdd:\\xbd\\xa3\\x9e\\xcb\\x85O}\\xaf\\xc7', 'keyId': 7501395501858684956}}, 'operationTime': Timestamp(1755838513, 27)}, acknowledged=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bulk delete all existing records from the collection defined above\n",
    "collection.delete_many({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d7a98f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingested 107 documents into the knowledge_base collection.\n"
     ]
    }
   ],
   "source": [
    "# Bulk insert `embedded_docs` into the `collection` defined above -- should be a one-liner\n",
    "collection.insert_many(embedded_docs)\n",
    "\n",
    "print(f\"Ingested {collection.count_documents({})} documents into the {COLLECTION_NAME} collection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1c00cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_index, check_index_ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9117ad5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the vector_index index\n",
      "vector_index index already exists, recreating...\n",
      "Dropping vector_index index\n",
      "Waiting for vector_index index deletion to complete...\n",
      "Waiting for vector_index index deletion to complete...\n",
      "Waiting for vector_index index deletion to complete...\n",
      "Waiting for vector_index index deletion to complete...\n",
      "Waiting for vector_index index deletion to complete...\n",
      "vector_index index deletion complete\n",
      "Creating new vector_index index\n",
      "Successfully recreated the vector_index index\n"
     ]
    }
   ],
   "source": [
    "# Define the model for the vector search index\n",
    "model = {\n",
    "\t\"name\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "\t\"type\": \"vectorSearch\",\n",
    "\t\"definition\": {\n",
    "\t\t\"fields\": [\n",
    "\t\t\t{\n",
    "\t\t\t\t\"type\": \"vector\",\n",
    "\t\t\t\t\"path\": \"embedding\",\n",
    "\t\t\t\t\"numDimensions\": 384,\n",
    "\t\t\t\t\"similarity\": \"cosine\"\n",
    "\t\t\t},\n",
    "\t\t\t{\"type\": \"filter\", \"path\": \"metadata.productName\"}\n",
    "\t\t]\n",
    "\t}\n",
    "}\n",
    "\n",
    "# Use the `create_index` function from the `utils` module to create a vector search index with the above definition for the `collection` collection\n",
    "create_index(collection, ATLAS_VECTOR_SEARCH_INDEX_NAME, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9f7dfde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector_index index status: PENDING\n",
      "vector_index index status: PENDING\n",
      "vector_index index status: PENDING\n",
      "vector_index index status: PENDING\n",
      "vector_index index status: READY\n",
      "vector_index index definition: {'fields': [{'type': 'vector', 'path': 'embedding', 'numDimensions': 384, 'similarity': 'cosine'}, {'type': 'filter', 'path': 'metadata.productName'}]}\n"
     ]
    }
   ],
   "source": [
    "# Use the `check_index_ready` function from the `utils` module to verify that the index was created and is in READY status before proceeding\n",
    "check_index_ready(collection, ATLAS_VECTOR_SEARCH_INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e87237de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking progress for task vs_index_creation\n"
     ]
    }
   ],
   "source": [
    "# Track progress of key steps-- DO NOT CHANGE\n",
    "track_progress(\"vs_index_creation\", \"ai_rag_lab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8d33db7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to retrieve relevant documents for a user query using vector search\n",
    "def vector_search(user_query: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents for a user query using vector search.\n",
    "\n",
    "    Args:\n",
    "    user_query (str): The user's query string.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of matching documents.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate embedding for the `user_query` using the `get_embedding` function defined in Step 4\n",
    "    query_embedding = get_embedding(user_query)\n",
    "\n",
    "    # Define an aggregation pipeline consisting of a $vectorSearch stage, followed by a $project stage\n",
    "    # Set the number of candidates to 150 and only return the top 5 documents from the vector search\n",
    "    # In the $project stage, exclude the `_id` field and include only the `body` field and `vectorSearchScore`\n",
    "    # NOTE: Use variables defined previously for the `index`, `queryVector` and `path` fields in the $vectorSearch stage\n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$vectorSearch\": {\n",
    "                \"index\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "                \"queryVector\": query_embedding,\n",
    "                \"path\": \"embedding\",\n",
    "                \"numCandidates\": 150,\n",
    "                \"limit\": 5\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$project\": {\n",
    "                \"_id\": 0,\n",
    "                \"body\": 1,\n",
    "                \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Execute the aggregation `pipeline` and store the results in `results`\n",
    "    results = collection.aggregate(pipeline)\n",
    "    return list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5f01c202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'body': '# Backup and Restore Sharded Clusters\\n\\nThe following tutorials describe backup and restoration for sharded clusters:\\n\\nTo use `mongodump` and `mongorestore` as a backup strategy for sharded clusters, you must stop the sharded cluster balancer and use the `fsync` command or the `db.fsyncLock()` method on `mongos` to block writes on the cluster during backups.\\n\\nSharded clusters can also use one of the following coordinated backup and restore processes, which maintain the atomicity guarantees of transactions across shards:\\n\\n- MongoDB Atlas\\n\\n- MongoDB Cloud Manager\\n\\n- MongoDB Ops Manager\\n\\nUse file system snapshots back up each component in the sharded cluster individually. The procedure involves stopping the cluster balancer. If your system configuration allows file system backups, this might be more efficient than using MongoDB tools.\\n\\nCreate backups using `mongodump` to back up each component in the cluster individually.',\n",
       "  'score': 0.9431124925613403},\n",
       " {'body': 'Create backups using `mongodump` to back up each component in the cluster individually.\\n\\nLimit the operation of the cluster balancer to provide a window for regular backup operations.\\n\\nAn outline of the procedure and consideration for restoring an *entire* sharded cluster from backup.',\n",
       "  'score': 0.9327977895736694},\n",
       " {'body': \"# Configuration and Maintenance\\n\\nThis section describes routine management operations, including updating your MongoDB deployment's configuration.\\n\\nOutlines common MongoDB configurations and examples of best-practice configurations for common use cases.\\n\\nUpgrade a MongoDB deployment to a different patch release within the same major release series.\\n\\nStart, configure, and manage running `mongod` process.\\n\\nStop in progress MongoDB client operations using `db.killOp()` and `maxTimeMS()`.\\n\\nArchive the current log files and start new ones.\",\n",
       "  'score': 0.9310877919197083},\n",
       " {'body': '## Full Time Diagnostic Data Capture\\n\\nTo help MongoDB engineers analyze server behavior, `mongod` and `mongos` processes include a Full Time Diagnostic Data Capture (FTDC) mechanism. FTDC is enabled by default. Due to its importance in debugging deployments, FTDC thread failures are fatal and stop the parent `mongod` or `mongos` process.\\n\\nFTDC data files are compressed and not human-readable. They inherit the same file access permissions as the MongoDB data files. Only users with access to FTDC data files can transmit the FTDC data.\\n\\nMongoDB engineers cannot access FTDC data without explicit permission and assistance from system owners or operators.\\n\\nFTDC data **never** contains any of the following information:\\n\\n- Samples of queries, query predicates, or query results\\n\\n- Data sampled from any end-user collection or index\\n\\n- System or MongoDB user credentials or security certificates',\n",
       "  'score': 0.9286065101623535},\n",
       " {'body': \"# Create a MongoDB Deployment\\n\\nYou can create a free tier MongoDB deployment on MongoDB Atlas to store and manage your data. MongoDB Atlas hosts and manages your MongoDB database in the cloud.\\n\\n## Create a Free MongoDB deployment on Atlas\\n\\nComplete the Get Started with Atlas guide to set up a new Atlas account and load sample data into a new free tier MongoDB deployment.\\n\\n## Save your Credentials\\n\\nAfter you create your database user, save that user's username and password to a safe location for use in an upcoming step.\\n\\nAfter you complete these steps, you have a new free tier MongoDB deployment on Atlas, database user credentials, and sample data loaded in your database.\\n\\nIf you run into issues on this step, ask for help in the MongoDB Community Forums or submit feedback by using the Rate this page tab on the right or bottom right side of this page.\",\n",
       "  'score': 0.9270492792129517}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_search(\"What are some best practices for data backups in MongoDB?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8cb35c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'body': '### Query Targeting\\n\\nQuery Targeting alerts often indicate inefficient queries.\\n\\nTo learn more, see Fix Query Issues.\\n\\n### Connection Limits\\n\\nConnection alerts typically occur when the maximum number of allowable connections to a MongoDB process has been exceeded. Once the limit is exceeded, no new connections can be opened until the number of open connections drops down below the limit.\\n\\nTo learn more, see Fix Connection Issues.\\n\\n## Alerts Workflow\\n\\nWhen an alert condition is met, the alert lifecycle begins.\\n\\nTo learn more, see the Alerts Workflow.',\n",
       "  'score': 0.9637913703918457},\n",
       " {'body': '# Resolve Alerts\\n\\nAtlas issues alerts for the database and server conditions configured in your alert settings. When a condition triggers an alert, Atlas displays a warning symbol on the cluster and sends alert notifications. Your alert settings determine the notification methods. Atlas continues sending notifications at regular intervals until the condition resolves or you delete or disable the alert. You should fix the immediate problem, implement a long-term solution, and view metrics to monitor your progress.\\n\\nIf you integrate with VictorOps, OpsGenie, or DataDog, you can recieve informational alerts from these third-party monitoring services in Atlas. However, you must resolve these alerts within each external service.\\n\\n<Tabs>\\n\\n<Tab name=\"Organization Alerts\">\\n\\n</Tab>\\n\\n<Tab name=\"Project Alerts\">\\n\\n</Tab>\\n\\n</Tabs>\\n\\n## View Alerts\\n\\n<Tabs>\\n\\n<Tab name=\"Organization Alerts\">',\n",
       "  'score': 0.9454998970031738},\n",
       " {'body': 'To configure these alert conditions, see Configure Alert Settings.\\n\\n### CPU Steal\\n\\nConfigure the alert settings to send an alert if this metric rises above 10%.\\n\\nTo configure this alert condition, see Configure Alert Settings.\\n\\n### Query Targeting\\n\\nConfigure the alert settings to send an alert if this metric rises above 50 or 100.\\n\\nTo configure these alert conditions, see Configure Alert Settings.\\n\\n### Connection Limits\\n\\nConfigure the alert settings to send an alert if the Connection % of the configured limit rises above 80% or 90%.\\n\\nTo configure these alert conditions, see Configure Alert Settings.\\n\\n## Resolve Alerts\\n\\nWhen a condition triggers an alert, Atlas displays a warning symbol on the cluster and sends alert notifications. Resolve these alerts and work to prevent alert conditions from occurring in the future. To learn how to fix the immediate problem, implement a long-term solution, and monitor your progress, see Resolve Alerts.\\n\\n### Tickets Available',\n",
       "  'score': 0.9340317845344543},\n",
       " {'body': '# Alert Basics\\n\\nAtlas provides built-in tools, alerts, charts, integrations, and logs to help you monitor your clusters. Atlas provides alerts to help you monitor your clusters and improve performance in the following ways:\\n\\n1. A variety of conditions can trigger an alert.\\n\\n2. You can configure alerts settings based on specific conditions for your databases, users, accounts, and more.\\n\\n3. When you resolve alerts, you can fix the immediate problem, implement a long-term solution, and monitor your progress.\\n\\nAtlas issues alerts for the database and server conditions configured in your alert settings. When a condition triggers an alert, Atlas displays a warning symbol on the cluster and sends alert notifications. Your alert settings determine the notification methods. Atlas continues sending notifications at regular intervals until the condition resolves or you delete or disable the alert.\\n\\n## Useful Metrics and Alert Conditions',\n",
       "  'score': 0.9327150583267212},\n",
       " {'body': 'To learn more, see the Connection alert conditions.\\n\\n## Configure Alerts\\n\\nTo set which conditions trigger alerts and how users are notified, Configure Alert Settings. You can configure alerts at the organization or project level. Atlas provides default alerts at the project level. You can clone existing alerts and configure maintenance window alerts.\\n\\nExperiment with alert condition values based on your specific requirements. Periodically reassess these values for optimal performance.\\n\\n### Tickets Available\\n\\nConfigure the alert settings to send an alert if these metrics drop below 30 for at least a few minutes. You want to avoid false positives triggered by relatively harmless short-term drops, but catch issues when these metrics stay low for a while.\\n\\nTo configure these alert conditions, see Configure Alert Settings.\\n\\n### Queues\\n\\nConfigure the alert settings to send an alert if these metrics rise above 100 for a minute. You want to avoid false positives triggered by relatively harmless short-term spikes, but catch issues when these metrics stay elevated for a while.',\n",
       "  'score': 0.9326193928718567}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_search(\"How to resolve alerts in MongoDB?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bdc1ddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the vector search index `model` from Step 6 to include the `metadata.productName` field as a `filter` field\n",
    "model = {\n",
    "    \"name\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "    \"type\": \"vectorSearch\",\n",
    "    \"definition\": {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"type\": \"vector\",\n",
    "                \"path\": \"embedding\",\n",
    "                \"numDimensions\": 384,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            {\"type\": \"filter\", \"path\": \"metadata.productName\"}\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a2ecdd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the vector_index index\n"
     ]
    }
   ],
   "source": [
    "# Use the `create_index` function from the `utils` module to re-create the vector search index with the modified model\n",
    "create_index(collection, ATLAS_VECTOR_SEARCH_INDEX_NAME, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1579e300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector_index index status: READY\n",
      "vector_index index definition: {'fields': [{'type': 'vector', 'path': 'embedding', 'numDimensions': 384, 'similarity': 'cosine'}, {'type': 'filter', 'path': 'metadata.productName'}]}\n"
     ]
    }
   ],
   "source": [
    "# Use the `check_index_ready` function from the `utils` module to verify that the index has the right filter fields and is in READY status before proceeding\n",
    "check_index_ready(collection, ATLAS_VECTOR_SEARCH_INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ff451cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the user query\n",
    "query_embedding = get_embedding(\n",
    "    \"What are some best practices for data backups in MongoDB?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "42e0cda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the aggregation pipeline defined in Step 7 to:\n",
    "# Include a filter in the $vectorSearch stage for documents where the `metadata.productName` field has the value \"MongoDB Atlas\".\n",
    "# Include the `metadata.productName` in the $project stage of the pipeline.\n",
    "pipeline = [\n",
    "    {\n",
    "        \"$vectorSearch\": {\n",
    "            \"index\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "            \"path\": \"embedding\",\n",
    "            \"queryVector\": query_embedding,\n",
    "            \"numCandidates\": 150,\n",
    "            \"limit\": 5,\n",
    "            \"filter\": {\"metadata.productName\": \"MongoDB Atlas\"}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$project\": {\n",
    "            \"_id\": 0,\n",
    "            \"body\": 1,\n",
    "            \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0242b344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'body': '# View Database Access History\\n\\n- This feature is not available for `M0` free clusters, `M2`, and `M5` clusters. To learn more, see Atlas M0 (Free Cluster), M2, and M5 Limits.\\n\\n- This feature is not supported on Serverless instances at this time. To learn more, see Serverless Instance Limitations.\\n\\n## Overview\\n\\nAtlas parses the MongoDB database logs to collect a list of authentication requests made against your clusters through the following methods:\\n\\n- `mongosh`\\n\\n- Compass\\n\\n- Drivers\\n\\nAuthentication requests made with API Keys through the Atlas Administration API are not logged.\\n\\nAtlas logs the following information for each authentication request within the last 7 days:\\n\\n<table>\\n<tr>\\n<th id=\"Field\">\\nField\\n\\n</th>\\n<th id=\"Description\">\\nDescription\\n\\n</th>\\n</tr>\\n<tr>\\n<td headers=\"Field\">\\nTimestamp',\n",
       "  'score': 0.9098455905914307},\n",
       " {'body': '### Query Targeting\\n\\nQuery Targeting alerts often indicate inefficient queries.\\n\\nTo learn more, see Fix Query Issues.\\n\\n### Connection Limits\\n\\nConnection alerts typically occur when the maximum number of allowable connections to a MongoDB process has been exceeded. Once the limit is exceeded, no new connections can be opened until the number of open connections drops down below the limit.\\n\\nTo learn more, see Fix Connection Issues.\\n\\n## Alerts Workflow\\n\\nWhen an alert condition is met, the alert lifecycle begins.\\n\\nTo learn more, see the Alerts Workflow.',\n",
       "  'score': 0.9058851003646851},\n",
       " {'body': '</td>\\n<td headers=\"Description\">\\nIndicates inefficient queries.\\n\\nThe change streams cursors that the Atlas Search process (`mongot`) uses to keep Atlas Search indexes updated can contribute to the query targeting ratio and trigger query targeting alerts if the ratio is high.\\n\\n</td>\\n</tr>\\n<tr>\\n<td headers=\"Alert%20Type\">\\nReplica Set Has No Primary\\n\\n</td>\\n<td headers=\"Description\">\\nNo primary is detected in replica set.\\n\\n</td>\\n</tr>\\n<tr>\\n<td headers=\"Alert%20Type\">\\nReplication Oplog Alerts\\n\\n</td>\\n<td headers=\"Description\">\\nAmount of oplog data generated on a primary cluster member is larger than the cluster\\'s configured oplog size.\\n\\n</td>\\n</tr>\\n<tr>\\n<td headers=\"Alert%20Type\">\\nSystem CPU Usage Alerts\\n\\n</td>\\n<td headers=\"Description\">\\nCPU usage of the MongoDB process reaches a specified threshold.\\n\\n</td>\\n</tr>\\n</table>',\n",
       "  'score': 0.8908545970916748},\n",
       " {'body': '</th>\\n<th id=\"Description\">\\nDescription\\n\\n</th>\\n</tr>\\n<tr>\\n<td headers=\"Alert%20Type\">\\nAtlas Search Alerts\\n\\n</td>\\n<td headers=\"Description\">\\nAmount of CPU and memory used by Atlas Search processes reach a specified threshold.\\n\\n</td>\\n</tr>\\n<tr>\\n<td headers=\"Alert%20Type\">\\nConnection Alerts\\n\\n</td>\\n<td headers=\"Description\">\\nNumber of connections to a MongoDB process exceeds the allowable maximum.\\n\\n</td>\\n</tr>\\n<tr>\\n<td headers=\"Alert%20Type\">\\nDisk Space % Used Alerts\\n\\n</td>\\n<td headers=\"Description\">\\nPercentage of used disk space on a partition reaches a specified threshold.\\n\\n</td>\\n</tr>\\n<tr>\\n<td headers=\"Alert%20Type\">\\nQuery Targeting Alerts\\n\\n</td>\\n<td headers=\"Description\">\\nIndicates inefficient queries.',\n",
       "  'score': 0.8889496326446533},\n",
       " {'body': '</td>\\n</tr>\\n<tr>\\n<td headers=\"Field\">\\nAuthentication Source\\n\\n</td>\\n<td headers=\"Description\">\\nThe database that the authentication request was made against. `admin` is the authentication source for SCRAM-SHA users and `$external` for LDAP users.\\n\\n</td>\\n</tr>\\n<tr>\\n<td headers=\"Field\">\\nAuthentication Result\\n\\n</td>\\n<td headers=\"Description\">\\nThe success or failure of the authentication request. A reason code is displayed for the failed authentication requests.\\n\\n</td>\\n</tr>\\n</table>Authentication requests are pre-sorted by descending timestamp with 25 entries per page.\\n\\n### Logging Limitations\\n\\nIf a cluster experiences an activity spike and generates an extremely large quantity of log messages, Atlas may stop collecting and storing new logs for a period of time.',\n",
       "  'score': 0.8866614699363708}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute the aggregation pipeline and view the results\n",
    "results = collection.aggregate(pipeline)\n",
    "list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "87736ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the vector search index `model` from Step 6 to include `metadata.contentType` and `updated` as `filter` fields\n",
    "model = {\n",
    "    \"name\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "    \"type\": \"vectorSearch\",\n",
    "    \"definition\": {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"type\": \"vector\",\n",
    "                \"path\": \"embedding\",\n",
    "                \"numDimensions\": 384,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            {\"type\": \"filter\", \"path\": \"metadata.contentType\"},\n",
    "            {\"type\": \"filter\", \"path\": \"updated\"}\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4e0fe9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the vector_index index\n",
      "vector_index index already exists, recreating...\n",
      "Dropping vector_index index\n",
      "Waiting for vector_index index deletion to complete...\n",
      "Waiting for vector_index index deletion to complete...\n",
      "Waiting for vector_index index deletion to complete...\n",
      "Waiting for vector_index index deletion to complete...\n",
      "Waiting for vector_index index deletion to complete...\n",
      "vector_index index deletion complete\n",
      "Creating new vector_index index\n",
      "Successfully recreated the vector_index index\n"
     ]
    }
   ],
   "source": [
    "# Use the `create_index` function from the `utils` module to re-create the vector search index with the modified model\n",
    "create_index(collection, ATLAS_VECTOR_SEARCH_INDEX_NAME, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7ff97174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector_index index status: PENDING\n",
      "vector_index index status: PENDING\n",
      "vector_index index status: PENDING\n",
      "vector_index index status: PENDING\n",
      "vector_index index status: READY\n",
      "vector_index index definition: {'fields': [{'type': 'vector', 'path': 'embedding', 'numDimensions': 384, 'similarity': 'cosine'}, {'type': 'filter', 'path': 'metadata.contentType'}, {'type': 'filter', 'path': 'updated'}]}\n"
     ]
    }
   ],
   "source": [
    "# Use the `check_index_ready` function from the `utils` module to verify that the index has the right filter fields and is in READY status before proceeding\n",
    "check_index_ready(collection, ATLAS_VECTOR_SEARCH_INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51852b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the user query\n",
    "query_embedding = get_embedding(\n",
    "    \"What are some best practices for data backups in MongoDB?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "27fce4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the aggregation pipeline defined in Step 7 to:\n",
    "# Include a filter in the $vectorSearch stage for documents where the `metadata.contentType` field is \"Tutorial\" AND the `updated` field is greater than or equal to \"2024-05-19\".\n",
    "# Include the `metadata.contentType` and `updated` fields in the $project stage of the pipeline.\n",
    "pipeline = [\n",
    "    {\n",
    "        \"$vectorSearch\": {\n",
    "            \"index\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "            \"path\": \"embedding\",\n",
    "            \"queryVector\": query_embedding,\n",
    "            \"numCandidates\": 150,\n",
    "            \"limit\": 5,\n",
    "            \"filter\": {\n",
    "                \"$and\": [\n",
    "                    {\"metadata.contentType\": \"Tutorial\"},\n",
    "                    {\"updated\": {\"$gte\": \"2024-05-19\"}}\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$project\": {\n",
    "            \"_id\": 0,\n",
    "            \"body\": 1,\n",
    "            \"updated\": 1,\n",
    "            \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fa222b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'updated': '2024-05-20T17:32:23.500Z',\n",
       "  'body': '**Getting the best recommendations**\\n\\nFor best results, follow these practices.\\n\\n - Give CodeWhisperer something to work with. The more code your file contains, the more context CodeWhisperer has for generating recommendations.\\n - Write descriptive comments in natural language  for example\\n```\\n// Take a JSON document as a String and store it in MongoDB returning the _id\\n```\\nOr\\n```\\n//Insert a document in a collection with a given _id and a discountLevel\\n```\\n - Specify the libraries you prefer at the start of your file by using import statements.\\n```\\n// This Java class works with MongoDB sync driver.\\n// This class implements Connection to MongoDB and CRUD methods.\\n```\\n - Use descriptive names for variables and functions\\n - Break down complex tasks into simpler tasks\\n\\n**Provide feedback**\\n----------------',\n",
       "  'score': 0.9205130338668823},\n",
       " {'updated': '2024-05-20T17:32:23.500Z',\n",
       "  'body': \"# Getting Started with MongoDB and AWS Codewhisperer\\n\\n**Introduction**\\n----------------\\n\\nAmazon CodeWhisperer is trained on billions of lines of code and can generate code suggestions  ranging from snippets to full functions  in real-time, based on your comments and existing code. AI code assistants have revolutionized developers coding experience, but what sets Amazon CodeWhisperer apart is that MongoDB has collaborated with the AWS Data Science team, enhancing its capabilities!\\n\\nAt MongoDB, we are always looking to enhance the developer experience, and we've fine-tuned the CodeWhisperer Foundational Models to deliver top-notch code suggestions  trained on, and tailored for, MongoDB. This gives developers of all levels the best possible experience when using CodeWhisperer for MongoDB functions.\",\n",
       "  'score': 0.9131487011909485},\n",
       " {'updated': '2024-05-20T17:32:23.500Z',\n",
       "  'body': 'const code = responseData.choices[0].message.content;\\n        // Get the required data to be added into the document\\n        const updateDoc = JSON.parse(code)\\n        // Set a flag that this document does not need further re-processing \\n        updateDoc.process = true\\n        await collection.updateOne({_id : docId}, {$set : updateDoc});\\n      \\n\\n    } else {\\n        console.error(\"Failed to generate filter JSON.\");\\n        console.log(JSON.stringify(responseData));\\n        return {};\\n    }\\n};\\n```\\n\\nKey steps include:',\n",
       "  'score': 0.9075396060943604},\n",
       " {'updated': '2024-05-20T17:32:23.500Z',\n",
       "  'body': '**Provide feedback**\\n----------------\\n\\nAs with all generative AI tools, they are forever learning and forever expanding their foundational knowledge base, and MongoDB is looking for feedback. If you are using Amazon CodeWhisperer in your MongoDB development, wed love to hear from you. \\n\\nWeve created a special codewhisperer tag on our [Developer Forums][6], and if you tag any post with this, it will be visible to our CodeWhisperer project team and we will get right on it to help and provide feedback. If you want to see what others are doing with CodeWhisperer on our forums, the [tag search link][7] will jump you straight into all the action. \\n\\nWe cant wait to see your thoughts and impressions of MongoDB and Amazon CodeWhisperer together.',\n",
       "  'score': 0.9073591232299805},\n",
       " {'updated': '2024-05-20T17:32:23.500Z',\n",
       "  'body': 'return sampledReviews;\\n}\\n```\\n\\n### Main trigger logic\\n\\nThe main trigger logic is invoked when an update change event is detected with a `\"process\" : false` field.\\n```javascript\\nexports = async function(changeEvent) {\\n  // A Database Trigger will always call a function with a changeEvent.\\n  // Documentation on ChangeEvents: https://www.mongodb.com/docs/manual/reference/change-events\\n\\n  // This sample function will listen for events and replicate them to a collection in a different Database\\nfunction sampleReviews(reviews) {\\n// Logic above...\\n   if (reviews.length <= 50) {\\n        return reviews;\\n    }\\n    const sampledReviews = [];\\n    const seenIndices = new Set();\\n\\n    while (sampledReviews.length < 50) {\\n        const randomIndex = Math.floor(Math.random() * reviews.length);\\n        if (!seenIndices.has(randomIndex)) {\\n            seenIndices.add(randomIndex);\\n            sampledReviews.push(reviews[randomIndex]);\\n        }\\n    }',\n",
       "  'score': 0.9065394997596741}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute the aggregation pipeline and view the results\n",
    "results = collection.aggregate(pipeline)\n",
    "list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fa956ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bb4f59f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create the user prompt for our RAG application\n",
    "def create_prompt(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a chat prompt that includes the user query and retrieved context.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's query string.\n",
    "\n",
    "    Returns:\n",
    "        str: The chat prompt string.\n",
    "    \"\"\"\n",
    "    # Retrieve the most relevant documents for the `user_query` using the `vector_search` function defined in Step 7\n",
    "    context = vector_search(user_query)\n",
    "    # Join the retrieved documents into a single string, where each document is separated by two new lines (\"\\n\\n\")\n",
    "    context = \"\\n\\n\".join([doc.get('body') for doc in context])\n",
    "    # Prompt consisting of the question and relevant context to answer it\n",
    "    prompt = f\"Answer the question based only on the following context. If the context is empty, say I DON'T KNOW\\n\\nContext:\\n{context}\\n\\nQuestion:{user_query}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2b6bb70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to answer user queries\n",
    "def generate_answer(user_query: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate an answer to the user query.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's query string.\n",
    "    \"\"\"\n",
    "    # Use the `create_prompt` function above to create a chat prompt\n",
    "    prompt = create_prompt(user_query)\n",
    "    # Format the message to the LLM in the format [{\"role\": <role_value>, \"content\": <content_value>}\n",
    "    # The role value for user messages must be \"user\"\n",
    "    # Use the `prompt` created above to populate the `content` field in the chat message\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    # Send the chat messages to a serverless function to get back an LLM response\n",
    "    response = requests.post(url=SERVERLESS_URL, json={\"task\": \"completion\", \"data\": messages})\n",
    "    # Print the final answer\n",
    "    print(response.json()[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "112ae46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, some best practices for data backups in MongoDB sharded clusters include:\n",
      "\n",
      "1. Stopping the sharded cluster balancer before performing backups.\n",
      "\n",
      "2. Using the `fsync` command or `db.fsyncLock()` method on `mongos` to block writes on the cluster during backups when using `mongodump` and `mongorestore`.\n",
      "\n",
      "3. Using coordinated backup and restore processes that maintain atomicity guarantees across shards, such as MongoDB Atlas, MongoDB Cloud Manager, or MongoDB Ops Manager.\n",
      "\n",
      "4. Using file system snapshots to back up each component in the sharded cluster individually, if the system configuration allows it.\n",
      "\n",
      "5. Using `mongodump` to back up each component in the cluster individually.\n",
      "\n",
      "6. Limiting the operation of the cluster balancer to provide a window for regular backup operations.\n",
      "\n",
      "7. For full sharded cluster backups, following a specific procedure and considerations for restoring the entire cluster from backup.\n",
      "\n",
      "These practices help ensure consistent and reliable backups of MongoDB sharded clusters.\n"
     ]
    }
   ],
   "source": [
    "generate_answer(\"What are some best practices for data backups in MongoDB?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "65e4a3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I DON'T KNOW. The context provided does not contain any question that you asked me. The context appears to be information about using CodeWhisperer and some JSON data, but there is no previous question included.\n"
     ]
    }
   ],
   "source": [
    "# Notice that the LLM does not remember the conversation history at this stage\n",
    "generate_answer(\"What did I just ask you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e74e56b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "89ef3a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Carlos Yovani\\projects\\practicWhitAI\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Carlos Yovani\\.cache\\huggingface\\hub\\models--mixedbread-ai--mxbai-rerank-xsmall-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "rerank_model = CrossEncoder(\"mixedbread-ai/mxbai-rerank-xsmall-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "39c39a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a re-ranking step to the following function\n",
    "def create_prompt(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a chat prompt that includes the user query and retrieved context.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's query string.\n",
    "\n",
    "    Returns:\n",
    "        str: The chat prompt string.\n",
    "    \"\"\"\n",
    "    # Retrieve the most relevant documents for the `user_query` using the `vector_search` function defined in Step 7\n",
    "    context = vector_search(user_query)\n",
    "    # Extract the \"body\" field from each document in `context`\n",
    "    documents = [d.get(\"body\") for d in context]\n",
    "    # Use the `rerank_model` instantiated above to re-rank `documents`\n",
    "    # Set the `top_k` argument to 5 \n",
    "    reranked_documents = rerank_model.rank(\n",
    "    user_query, documents, return_documents=True, top_k=5\n",
    ")\n",
    "    # Join the re-ranked documents into a single string, where each document is separated by two new lines (\"\\n\\n\")\n",
    "    context = \"\\n\\n\".join([d.get(\"text\", \"\") for d in reranked_documents])\n",
    "    # Prompt consisting of the question and relevant context to answer it\n",
    "    prompt = f\"Answer the question based only on the following context. If the context is empty, say I DON'T KNOW\\n\\nContext:\\n{context}\\n\\nQuestion:{user_query}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "73a0f550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, some best practices for data backups in MongoDB sharded clusters include:\n",
      "\n",
      "1. Stopping the sharded cluster balancer before performing backups.\n",
      "\n",
      "2. Using the `fsync` command or `db.fsyncLock()` method on `mongos` to block writes during backups when using `mongodump` and `mongorestore`.\n",
      "\n",
      "3. Using coordinated backup and restore processes that maintain atomicity guarantees across shards, such as MongoDB Atlas, MongoDB Cloud Manager, or MongoDB Ops Manager.\n",
      "\n",
      "4. Using file system snapshots to back up each component in the sharded cluster individually, if the system configuration allows.\n",
      "\n",
      "5. Using `mongodump` to back up each component in the cluster individually.\n",
      "\n",
      "6. Limiting the operation of the cluster balancer to provide a window for regular backup operations.\n",
      "\n",
      "7. For full sharded cluster backups, following a specific procedure and considerations for restoring the entire cluster.\n",
      "\n",
      "The context also mentions that it's important to consider the atomicity guarantees of transactions across shards when choosing a backup method.\n"
     ]
    }
   ],
   "source": [
    "# Note the impact of re-ranking on the generated answer\n",
    "# You might not see a difference in this example since we are only re-ranking 5 documents\n",
    "# In practice, you would send a larger number of documents to the re-ranker, and get the top few AFTER reranking\n",
    "generate_answer(\"What are some best practices for data backups in MongoDB?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c6b4e156",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f7b7b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_collection = mongodb_client[DB_NAME][\"chat_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2c35bb5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'session_id_1'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an index on the key `session_id` for the `history_collection` collection\n",
    "history_collection.create_index(\"session_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dfe1e634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_chat_message(session_id: str, role: str, content: str) -> None:\n",
    "    \"\"\"\n",
    "    Store a chat message in a MongoDB collection.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Session ID of the message.\n",
    "        role (str): Role for the message. One of `system`, `user` or `assistant`.\n",
    "        content (str): Content of the message.\n",
    "    \"\"\"\n",
    "    # Create a message object with `session_id`, `role`, `content` and `timestamp` fields\n",
    "    # `timestamp` should be set the current timestamp\n",
    "    message = {\n",
    "        \"session_id\": session_id,\n",
    "        \"role\": role,\n",
    "        \"content\": content,\n",
    "        \"timestamp\": datetime.now(),\n",
    "    }\n",
    "    # Insert the `message` into the `history_collection` collection\n",
    "    history_collection.insert_one(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fd6de291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_session_history(session_id: str) -> List:\n",
    "    \"\"\"\n",
    "    Retrieve chat message history for a particular session.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Session ID to retrieve chat message history for.\n",
    "\n",
    "    Returns:\n",
    "        List: List of chat messages.\n",
    "    \"\"\"\n",
    "    # Query the `history_collection` collection for documents where the \"session_id\" field has the value of the input `session_id`\n",
    "    # Sort the results in increasing order of the values in `timestamp` field\n",
    "    cursor =  history_collection.find({\"session_id\": session_id}).sort(\"timestamp\", 1)\n",
    "\n",
    "    if cursor:\n",
    "        # Iterate through the cursor and extract the `role` and `content` field from each entry\n",
    "        # Then format each entry as: {\"role\": <role_value>, \"content\": <content_value>}\n",
    "        messages = [{\"role\": msg[\"role\"], \"content\": msg[\"content\"]} for msg in cursor]\n",
    "    else:\n",
    "        # If cursor is empty, return an empty list\n",
    "        messages = []\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6d334830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(session_id: str, user_query: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate an answer to the user's query taking chat history into account.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Session ID to retrieve chat history for.\n",
    "        user_query (str): The user's query string.\n",
    "    \"\"\"\n",
    "    # Initialize list of messages to pass to the chat completion model\n",
    "    messages = []\n",
    "\n",
    "    # Retrieve documents relevant to the user query and convert them to a single string\n",
    "    context = vector_search(user_query)\n",
    "    context = \"\\n\\n\".join([d.get(\"body\", \"\") for d in context])\n",
    "    # Create a system prompt containing the retrieved context\n",
    "    system_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Answer the question based only on the following context. If the context is empty, say I DON'T KNOW\\n\\nContext:\\n{context}\",\n",
    "    }\n",
    "    # Append the system prompt to the `messages` list\n",
    "    messages.append(system_message)\n",
    "\n",
    "    # Use the `retrieve_session_history` function to retrieve message history from MongoDB for the session ID `session_id` \n",
    "    # And add all messages in the message history to the `messages` list \n",
    "    message_history = retrieve_session_history(session_id)\n",
    "    messages.extend(message_history)\n",
    "\n",
    "    # Format the user query in the format {\"role\": <role_value>, \"content\": <content_value>}\n",
    "    # The role value for user messages must be \"user\"\n",
    "    # And append the user message to the `messages` list\n",
    "    user_message = {\"role\": \"user\", \"content\": user_query}\n",
    "    messages.append(user_message)\n",
    "\n",
    "    # Send the chat messages to a serverless function to get back an LLM response\n",
    "    response = requests.post(url=SERVERLESS_URL, json={\"task\": \"completion\", \"data\": messages})\n",
    "\n",
    "    # Extract the answer from the response\n",
    "    answer = response.json()[\"text\"]\n",
    "\n",
    "    # Use the `store_chat_message` function to store the user message and also the generated answer in the message history collection\n",
    "    # The role value for user messages is \"user\", and \"assistant\" for the generated answer\n",
    "    store_chat_message(session_id, \"user\", user_query)\n",
    "    store_chat_message(session_id, \"assistant\", answer)\n",
    "\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "37b8082f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based solely on the provided context, the answer is:\n",
      "\n",
      "I DON'T KNOW\n",
      "\n",
      "The given context does not contain specific information about best practices for data backups in MongoDB. While it mentions some backup-related topics for sharded clusters, it does not provide a comprehensive list of best practices for MongoDB backups in general.\n"
     ]
    }
   ],
   "source": [
    "generate_answer(\n",
    "    session_id=\"1\",\n",
    "    user_query=\"What are some best practices for data backups in MongoDB?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d98f95dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You just asked: \"What are some best practices for data backups in MongoDB?\"\n"
     ]
    }
   ],
   "source": [
    "generate_answer(\n",
    "    session_id=\"1\",\n",
    "    user_query=\"What did I just ask you?\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
